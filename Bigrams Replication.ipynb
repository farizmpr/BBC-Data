{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8aa88d8",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97a5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c53df",
   "metadata": {},
   "source": [
    "### Predefined the Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ff6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predefined bigrams related to climate change (C₀)\n",
    "predefined_bigrams_C0 = [\n",
    "    (\"air\", \"pollution\"),\n",
    "    (\"air\", \"quality\"),\n",
    "    (\"air\", \"temperature\"),\n",
    "    (\"biomass\", \"energy\"),\n",
    "    (\"carbon\", \"dioxide\"),\n",
    "    (\"carbon\", \"emission\"),\n",
    "    (\"carbon\", \"energy\"),\n",
    "    (\"carbon\", \"neutral\"),\n",
    "    (\"carbon\", \"price\"),\n",
    "    (\"carbon\", \"sink\"),\n",
    "    (\"carbon\", \"tax\"),\n",
    "    (\"clean\", \"air\"),\n",
    "    (\"clean\", \"energy\"),\n",
    "    (\"clean\", \"water\"),\n",
    "    (\"climate\", \"change\"),\n",
    "    (\"coastal\", \"area\"),\n",
    "    (\"coastal\", \"region\"),\n",
    "    (\"electric\", \"vehicle\"),\n",
    "    (\"energy\", \"climate\"),\n",
    "    (\"energy\", \"conversion\"),\n",
    "    (\"energy\", \"efficient\"),\n",
    "    (\"energy\", \"environment\"),\n",
    "    (\"environmental\", \"sustainability\"),\n",
    "    (\"extreme\", \"weather\"),\n",
    "    (\"flue\", \"gas\"),\n",
    "    (\"forest\", \"land\"),\n",
    "    (\"gas\", \"emission\"),\n",
    "    (\"ghg\", \"emission\"),\n",
    "    (\"global\", \"decarbonization\"),\n",
    "    (\"global\", \"warm\"),\n",
    "    (\"greenhouse\", \"gas\"),\n",
    "    (\"heat\", \"power\"),\n",
    "    (\"Kyoto\", \"protocol\"),\n",
    "    (\"natural\", \"hazard\"),\n",
    "    (\"new\", \"energy\"),\n",
    "    (\"ozone\", \"layer\"),\n",
    "    (\"renewable\", \"energy\"),\n",
    "    (\"sea\", \"level\"),\n",
    "    (\"sea\", \"water\"),\n",
    "    (\"snow\", \"ice\"),\n",
    "    (\"solar\", \"energy\"),\n",
    "    (\"solar\", \"thermal\"),\n",
    "    (\"sustainable\", \"energy\"),\n",
    "    (\"water\", \"resource\"),\n",
    "    (\"water\", \"resources\"),\n",
    "    (\"wave\", \"energy\"),\n",
    "    (\"weather\", \"climate\"),\n",
    "    (\"wind\", \"energy\"),\n",
    "    (\"wind\", \"power\"),\n",
    "    (\"wind\", \"resource\"),\n",
    "    \n",
    "    # Bigrams from Table IA. IV - Panel A (Opportunity Bigrams)\n",
    "    (\"heat\", \"power\"),\n",
    "    (\"new\", \"energy\"),\n",
    "    (\"plug\", \"hybrid\"),\n",
    "    (\"rooftop\", \"solar\"),\n",
    "    (\"renewable\", \"electricity\"),\n",
    "    (\"renewable\", \"energy\"),\n",
    "    (\"wind\", \"power\"),\n",
    "    (\"renewable\", \"resource\"),\n",
    "    (\"solar\", \"farm\"),\n",
    "    (\"sustainable\", \"energy\"),\n",
    "    (\"electric\", \"vehicle\"),\n",
    "    (\"wind\", \"energy\"),\n",
    "    (\"solar\", \"energy\"),\n",
    "    (\"hybrid\", \"car\"),\n",
    "    (\"clean\", \"energy\"),\n",
    "    (\"electric\", \"hybrid\"),\n",
    "    (\"geothermal\", \"power\"),\n",
    "    \n",
    "    # Bigrams from Table IA. IV - Panel B (Regulatory Bigrams)\n",
    "    (\"greenhouse\", \"gas\"),\n",
    "    (\"gas\", \"emission\"),\n",
    "    (\"carbon\", \"tax\"),\n",
    "    (\"emission\", \"trade\"),\n",
    "    (\"carbon\", \"reduction\"),\n",
    "    (\"reduce\", \"emission\"),\n",
    "    (\"air\", \"pollution\"),\n",
    "    (\"carbon\", \"price\"),\n",
    "    (\"dioxide\", \"emission\"),\n",
    "    (\"carbon\", \"market\"),\n",
    "    (\"carbon\", \"emission\"),\n",
    "    (\"reduce\", \"carbon\"),\n",
    "    (\"environmental\", \"standard\"),\n",
    "    (\"epa\", \"regulation\"),\n",
    "    (\"mercury\", \"emission\"),\n",
    "    (\"carbon\", \"dioxide\"),\n",
    "    (\"energy\", \"regulatory\"),\n",
    "    (\"nox\", \"emission\"),\n",
    "    (\"energy\", \"independence\"),\n",
    "    \n",
    "    # Bigrams from Table IA. IV - Panel C (Physical Bigrams)\n",
    "    (\"coastal\", \"area\"),\n",
    "    (\"forest\", \"land\"),\n",
    "    (\"storm\", \"water\"),\n",
    "    (\"natural\", \"hazard\"),\n",
    "    (\"water\", \"discharge\"),\n",
    "    (\"global\", \"warm\"),\n",
    "    (\"sea\", \"level\"),\n",
    "    (\"heavy\", \"snow\"),\n",
    "    (\"sea\", \"water\"),\n",
    "    (\"ice\", \"product\"),\n",
    "    (\"snow\", \"ice\"),\n",
    "    (\"nickel\", \"metal\"),\n",
    "    (\"air\", \"water\"),\n",
    "    (\"warm\", \"climate\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e60ee",
   "metadata": {},
   "source": [
    "### Loading IPCC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d44627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all IPCC reports into a single text string.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the IPCC report text files\n",
    "directory = r'C:\\Users\\fariz\\Documents\\Graduate Data\\Climate Change\\Fix Data\\IPCC\\IPCC\\raw_txt\\IPCC'\n",
    "\n",
    "# Initialize an empty string to hold all text\n",
    "ipcc_text = \"\"\n",
    "\n",
    "# Loop through each file in the directory and read the text\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            ipcc_text += file.read() + \"\\n\"\n",
    "\n",
    "print(\"Loaded all IPCC reports into a single text string.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1976ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for  \n",
      "Policymakers\n",
      "SPM\n",
      "3\n",
      "Summary \n",
      "for Policymakers\n",
      "Drafting Authors: \n",
      "Nerilie Abram (Australia), Carolina Adler (Switzerland/Australia), Nathaniel L. Bindoff (Australia), \n",
      "Lijing Cheng (China), So-Min Cheong (Republic of Korea), William  W.  L. Cheung (Canada), \n",
      "Matthew Collins (UK), Chris Derksen (Canada), Alexey Ekaykin (Russian Federation), Thomas \n",
      "Frölicher (Switzerland), Matthias Garschagen (Germany), Jean-Pierre Gattuso (France), Bruce \n",
      "Glavovic (New Zealand), Stephan Gruber (Canad\n"
     ]
    }
   ],
   "source": [
    "print(ipcc_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce99dbb",
   "metadata": {},
   "source": [
    "## lemmatize and stem the textual IPCC data, removing digits, punctuation, and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c4f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(ipcc_text.lower())\n",
    "\n",
    "# Lemmatize, stem, and clean the tokens\n",
    "cleaned_words = [\n",
    "    stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens \n",
    "    if word not in stopwords.words('english') and word not in string.punctuation and not word.isdigit()\n",
    "]\n",
    "\n",
    "print(\"Text preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18febcd",
   "metadata": {},
   "source": [
    "#### Filtering Bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library Bigrams\n",
    "from nltk import bigrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22473bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Bigrams: 181168 bigrams with frequency > 10\n"
     ]
    }
   ],
   "source": [
    "# Generate bigrams\n",
    "ipcc_bigrams = list(bigrams(cleaned_words))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "bigram_freq = Counter(ipcc_bigrams)\n",
    "\n",
    "# Filter bigrams with a frequency higher than 10\n",
    "filtered_bigrams = {bigram: count for bigram, count in bigram_freq.items() if count > 10}\n",
    "\n",
    "print(f\"Filtered Bigrams: {len(filtered_bigrams)} bigrams with frequency > 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dd35b",
   "metadata": {},
   "source": [
    "### Preprocessing the Non-climate-change texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6f6047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all non-climate text into a single string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 517633 bigrams for non-climate-change text.\n"
     ]
    }
   ],
   "source": [
    "# Set the directory where your BBC data is stored\n",
    "directory = r'C:\\Users\\fariz\\Documents\\Graduate Data\\Climate Change\\Model\\bbc-fulltext\\bbc'\n",
    "\n",
    "# Initialize an empty string to hold all non-climate text\n",
    "non_climate_text = \"\"\n",
    "\n",
    "# Loop through each subdirectory (business, entertainment, politics, etc.)\n",
    "for subdir in os.listdir(directory):\n",
    "    subdir_path = os.path.join(directory, subdir)\n",
    "    if os.path.isdir(subdir_path):  # Check if it is a directory\n",
    "        # Loop through each text file in the subdirectory\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(subdir_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    non_climate_text += file.read() + \"\\n\"\n",
    "\n",
    "print(\"Loaded all non-climate text into a single string.\")\n",
    "\n",
    "# Preprocess the non-climate text\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize and clean the non-climate-change text\n",
    "non_climate_tokens = word_tokenize(non_climate_text.lower())\n",
    "\n",
    "# Lemmatize, stem, and clean the tokens\n",
    "cleaned_non_climate_words = [\n",
    "    stemmer.stem(lemmatizer.lemmatize(word)) for word in non_climate_tokens \n",
    "    if word not in stopwords.words('english') and word not in string.punctuation and not word.isdigit()\n",
    "]\n",
    "\n",
    "# Generate bigrams for non-climate-change text\n",
    "non_climate_bigrams = list(nltk.bigrams(cleaned_non_climate_words))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "non_climate_bigram_freq = Counter(non_climate_bigrams)\n",
    "\n",
    "print(f\"Generated {len(non_climate_bigrams)} bigrams for non-climate-change text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61da05",
   "metadata": {},
   "source": [
    "### Identify the bigrams that contaminated by unrelated climate change topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4a0b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 166328 unique climate-change bigrams.\n"
     ]
    }
   ],
   "source": [
    "# Identify bigrams that are in Cᵀ (climate change set) but not in N (non-climate change set)\n",
    "unique_climate_bigrams = {bigram: count for bigram, count in filtered_bigrams.items() if bigram not in non_climate_bigram_freq}\n",
    "\n",
    "print(f\"Identified {len(unique_climate_bigrams)} unique climate-change bigrams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b70056",
   "metadata": {},
   "source": [
    "### Create bigram set M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a42dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipcc_sentences = nltk.sent_tokenize(ipcc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2321bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sentences to create set M\n",
    "M = [sentence for sentence in ipcc_sentences if any(bigram in list(bigrams(sentence.split())) for bigram in unique_climate_bigrams)]\n",
    "\n",
    "print(f\"Filtered set M contains {len(M)} sentences likely discussing climate change.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b871e",
   "metadata": {},
   "source": [
    "### Reference and search set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference set R\n",
    "R = [sentence for sentence in M if any(bigram in list(bigrams(sentence.split())) for bigram in predefined_bigrams_C0)]\n",
    "\n",
    "print(f\"Reference set R contains {len(R)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set S\n",
    "S = [sentence for sentence in M if sentence not in R]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbff5a",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99819dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd53d18",
   "metadata": {},
   "source": [
    "### Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8714ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from S to create balanced training data\n",
    "sampled_S = random.sample(S, len(R))\n",
    "\n",
    "# Combine R and sampled S into a training set\n",
    "training_sentences = R + sampled_S\n",
    "labels = [1] * len(R) + [0] * len(sampled_S)\n",
    "\n",
    "# Convert text data into numerical features\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Use unigrams and bigrams\n",
    "X = vectorizer.fit_transform(training_sentences)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_preds = nb.predict(X_test)\n",
    "\n",
    "# SVM\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_preds = svm.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_preds))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_preds))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification and prediction\n",
    "# Predict probabilities for search set S\n",
    "S_vectorized = vectorizer.transform(S)\n",
    "nb_probs = nb.predict_proba(S_vectorized)[:, 1]  # Probability of belonging to R\n",
    "svm_probs = svm.decision_function(S_vectorized)  # SVM decision function\n",
    "rf_probs = rf.predict_proba(S_vectorized)[:, 1]  # Random Forest probability\n",
    "\n",
    "# Combine predictions and create target set T\n",
    "threshold = 0.8\n",
    "T = [S[i] for i in range(len(S)) if nb_probs[i] > threshold or svm_probs[i] > threshold or rf_probs[i] > threshold]\n",
    "\n",
    "print(f\"Target set T contains {len(T)} sentences likely discussing climate change.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c66f3a",
   "metadata": {},
   "source": [
    "### Final Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c98321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and compare bigrams from T and S \\ T\n",
    "T_bigrams = Counter([bigram for sentence in T for bigram in list(bigrams(preprocess(sentence)))])\n",
    "S_minus_T_bigrams = Counter([bigram for sentence in (set(S) - set(T)) for bigram in list(bigrams(preprocess(sentence)))])\n",
    "\n",
    "# Discriminative bigrams: more frequent in T than S \\ T\n",
    "discriminative_bigrams = {bigram: count for bigram, count in T_bigrams.items() if count > S_minus_T_bigrams.get(bigram, 0)}\n",
    "\n",
    "# Rank and finalize the bigram set\n",
    "final_bigrams = sorted(discriminative_bigrams, key=discriminative_bigrams.get, reverse=True)\n",
    "\n",
    "print(f\"Final bigram library contains {len(final_bigrams)} bigrams.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
